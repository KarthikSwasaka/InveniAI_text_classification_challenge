It consist of
1.Spliting the dataset into training and test sets as appropriate.
2. Performing exploratory data analysis on the data.
3. Building a text classification model based on the dataset.
4. Monitoring relevant evaluation metrics while training the model.
5. Presenting any interesting or important findings from the exploratory data analysis.
6. Saving the trained NLP model and make it available through an API.
7. Dockerizing the API.
8. Hosting a local RabbitMQ message queue server and sending data for inference through the message queue.
9. Creating a RabbitMQ Consumer that consumes data from the queue and makes an API call for model inference.
10. Saving the inference results along with the text on which model inference was made in a Postgres Database (which can be locally hosted on my machine)

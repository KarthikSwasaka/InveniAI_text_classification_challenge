It consist of
1.Spliting the dataset into training and test sets as appropriate./n
2. Performing exploratory data analysis on the data./n
3. Building a text classification model based on the dataset. /n
4. Monitoring relevant evaluation metrics while training the model /n
5. Presenting any interesting or important findings from the exploratory data analysis./n
6. Saving the trained NLP model and make it available through an API. /n
7. Dockerizing the API. /n
8. Hosting a local RabbitMQ message queue server and sending data for inference through the message queue. /n
9. Creating a RabbitMQ Consumer that consumes data from the queue and makes an API call for model inference. /n
10. Saving the inference results along with the text on which model inference was made in a Postgres Database (which can be locally hosted on my machine).
